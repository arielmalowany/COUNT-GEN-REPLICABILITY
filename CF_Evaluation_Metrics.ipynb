{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6372ace",
   "metadata": {},
   "source": [
    "### Import libraries and custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a3ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 112 layers, 68,125,494 parameters, 0 gradients, 257.4 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import os\n",
    "import cv2\n",
    "from jmetal.algorithm.multiobjective import NSGAII\n",
    "from jmetal.operator import SBXCrossover, PolynomialMutation\n",
    "from jmetal.util.termination_criterion import StoppingByEvaluations\n",
    "import matplotlib.pyplot as plt\n",
    "from jmetal.lab.experiment import Experiment, Job, generate_summary_from_experiment\n",
    "from jmetal.core.quality_indicator import HyperVolume\n",
    "import openpyxl\n",
    "\n",
    "# custom scripts\n",
    "from my_attgan import AttGAN\n",
    "from my_mivolo_inference import mivolo_inference\n",
    "from my_mivolo_inference import predictor\n",
    "from cf_utils import *\n",
    "from generate_gender_cfs import AttGanPlausibleCounterfactualProblem\n",
    "from data import Custom\n",
    "from data import CelebA_HQ_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ec3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings\n",
    "\n",
    "image_size = 384\n",
    "with open('./384_shortcut1_inject1_none_hq/setting.txt', 'r') as f:\n",
    "    gan_args = json.load(f)\n",
    "    \n",
    "training_set_size = 1000\n",
    "training_set_image_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b031c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AttGAN\n",
    "\n",
    "attgan = AttGAN(gan_args)\n",
    "attgan.load('./384_shortcut1_inject1_none_hq/weights.149.pth')\n",
    "attgan.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c3c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CelebaHQ N samples\n",
    "\n",
    "celeba_path = './celeba_hq_dataset/CelebA-HQ-img'\n",
    "atts_path = './celeba_hq_dataset/CelebAMask-HQ-attribute-anno.txt'\n",
    "base_attrs = gan_args.get('attrs')\n",
    "\n",
    "sample_celeba_data = CelebA_HQ_custom(\n",
    "                       data_path = celeba_path,\n",
    "                       attr_path = atts_path,\n",
    "                       selected_attrs = base_attrs,\n",
    "                       image_size = training_set_image_size,\n",
    "                       mode = 'train'\n",
    "                     )\n",
    "\n",
    "sample_celeba_dataloader = data.DataLoader(\n",
    "                             sample_celeba_data, batch_size=training_set_size, num_workers=gan_args.get('num_workers'),\n",
    "                             shuffle=True, drop_last=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7c9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load valid CFs\n",
    "\n",
    "path = './Counterfactuals/Front_22340_0.pkl'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "  pareto_front = pkl.load(f)\n",
    "  factual_img = pkl.load(f)\n",
    "  factual_atts = pkl.load(f)\n",
    "  runtime_in_seconds = pkl.load(f)\n",
    "  experiment_metadata = pkl.load(f)\n",
    "  \n",
    "raw_x_data, raw_y_data, raw_z_data, new_preds, new_attributes, generated_cfs, dominance_ranking, crowding_distances = unpack_front(pareto_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c127e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid CFs\n",
    "\n",
    "valid_cfs_idx = [i for i, y in enumerate(raw_y_data) if y < 0.5]\n",
    "valid_cf_images = torch.stack([generated_cfs[i] for i in valid_cfs_idx]).squeeze(1)\n",
    "\n",
    "count_valid_cfs = len([y for y in raw_y_data if y < 0.5])\n",
    "valid_cf_atts = torch.stack([new_attributes[i] for i in valid_cfs_idx])\n",
    "\n",
    "raw_x_data = [raw_x_data[i] for i in valid_cfs_idx]\n",
    "raw_y_data = [raw_y_data[i] for i in valid_cfs_idx]\n",
    "raw_z_data = [raw_z_data[i] for i in valid_cfs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define factual image \n",
    "\n",
    "factual_image = factual_img\n",
    "factual_gender_prob, factual_age = mivolo_inference(factual_image, True)\n",
    "if factual_gender_prob > 0.5:\n",
    "  factual_gender_binary = 1\n",
    "else:\n",
    "  factual_gender_binary = 0\n",
    "  \n",
    "# Obtain the predicted attributes for the factual image\n",
    "\n",
    "with torch.no_grad():\n",
    "  factual_d_score, dc_real = attgan.D(factual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5b746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch of N random images\n",
    "\n",
    "data_iterator = iter(sample_celeba_dataloader)\n",
    "training_set_images, training_set_attributes, training_set_names = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef825bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Metrics Dictionary\n",
    "\n",
    "evaluation_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "pop_size = len(generated_cfs)\n",
    "\n",
    "###  SIZE #### \n",
    "\n",
    "size = count_valid_cfs / pop_size\n",
    "\n",
    "evaluation_metrics[\"Size\"] = {\n",
    "  \"size_pct\":{\"value\":size, \"definition\":\"Available CFs wrt required CFs\"}, \n",
    "  \"size_count\":{\"value\":count_valid_cfs, \"definition\":\"Available CFs\"}, \n",
    "  \"required_cfs\":{\"value\":pop_size, \"definition\":\"Required CFs\"}\n",
    "  }\n",
    "\n",
    "### PROXIMITY ###\n",
    "\n",
    "# Proximity - features\n",
    "\n",
    "avg_distance = np.mean(raw_x_data) # code difference\n",
    "\n",
    "# Proximity - images\n",
    "\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', reduction='none') # LPIPS needs the images to be in the [-1, 1] range.\n",
    "lpips_score = 0\n",
    "for cf in valid_cf_images:\n",
    "  lpips_score += lpips(factual_image, cf.unsqueeze(0))\n",
    "lpips_score /= count_valid_cfs\n",
    "\n",
    "# Minimality - average number of features changed\n",
    "\n",
    "features_changed = torch.abs(valid_cf_atts - dc_real)/dc_real > 0.05\n",
    "avg_features_changed = torch.sum(torch.sum(features_changed, axis = 1)) / (features_changed.shape[0] * features_changed.shape[1])\n",
    "\n",
    "evaluation_metrics[\"Dissimilarity\"] = {\n",
    "  \"proximity\":{\n",
    "    \"avg_distance_features\":{\"value\":round(avg_distance.item(), 3), \"definition\":\"Average distance between the factual image features and valid CFs features (distance used for the code similarity objective)\"}, \n",
    "    \"avg_distance_images\":{\"value\":round(lpips_score.item(), 3), \"definition\":\"Average LPIPS score between the factual image and valid CFs\"}\n",
    "   },\n",
    "  \"minimality\":{\"avg_features_changed\":{\"value\":round(avg_features_changed.item(), 3), \"definition\":\"Average number of features changed for valid CFs\"}}\n",
    "  }\n",
    "\n",
    "### DIVERSITY ###\n",
    "\n",
    "# Diversity - average distance between attributes\n",
    "\n",
    "distances = torch.sqrt(((valid_cf_atts - dc_real) ** 2).sum(-1))\n",
    "mean_distance = torch.mean(distances)\n",
    "\n",
    "# Diversity - standard deviation for each attribute\n",
    "\n",
    "attribute_deviations = torch.std(valid_cf_atts, dim = 0)\n",
    "attribute_means = torch.mean(valid_cf_atts, dim = 0)\n",
    "mean_attribute_value = torch.mean(attribute_means)\n",
    "std_attribute_value = torch.mean(attribute_deviations)\n",
    "\n",
    "# Diversity - LPIPS distance between images\n",
    "\n",
    "average_lpips_distance = 0\n",
    "for cf in valid_cf_images: # n\n",
    "  average_lpips_distance += torch.sum(distances_between_image_sets(cf, 128, valid_cf_images, 'lpips')[0][1:]) # (n - 1 sumas)\n",
    "average_lpips_distance /= (count_valid_cfs * (count_valid_cfs - 1))\n",
    "  \n",
    "# Diversity - Euclidean distance between images  \n",
    "  \n",
    "mean_cf_pixelwise_distance = 0\n",
    "for cf in valid_cf_images: # n\n",
    "  mean_cf_pixelwise_distance += torch.sum(distances_between_image_sets(cf, 128, valid_cf_images, 'pixel_wise')[0][1:]) # (n - 1 sumas)\n",
    "mean_cf_pixelwise_distance /= (count_valid_cfs * (count_valid_cfs - 1))\n",
    "\n",
    "evaluation_metrics[\"Diversity\"] = {\n",
    "    \"avg_distance_features\":{\"value\":round(avg_distance.item(), 3), \"definition\":\"Average distance between the factual image features and valid CFs features (distance used for the code similarity objective)\"}, \n",
    "    \"avg_distance_features_L2\":{\"value\":round(mean_distance.item(), 3), \"definition\":\"Average distance between the factual image features and valid CFs features (L2 distance)\"},\n",
    "    \"mean_cf_pixelwise_distance\":{\"value\":round(mean_cf_pixelwise_distance.item(), 3), \"definition\":\"Average pixel-wise distance of the CF set\"},\n",
    "    \"mean_cf_lpips_distance\":{\"value\":round(average_lpips_distance.item(), 3), \"definition\":\"Average LPIPS distance of the CF set\"},\n",
    "    \"mean_std\":{\"value\":round(std_attribute_value.item(), 3), \"definition\":\"Std deviation across all CF's attributes\"},\n",
    "    \"attribute_means\":{\"value\":attribute_means, \"definition\":\"Average value of CF's attributes\"},\n",
    "    \"attribute_changes\":{\"value\":torch.sum(features_changed, axis = 0)/count_valid_cfs, \"definition\":\"Number of CFs that had changed the attribute's value\"},\n",
    "    \"attribute_deviations\":{\"value\":attribute_deviations, \"definition\":\"Standard deviation of CF's attributes\"},\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf00db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminative Power - Distinguish between two different classes only using the CFs in C\n",
    "\n",
    "# Test set - Gender Prediction\n",
    "\n",
    "def gender_training_set(training_set = training_set_images, threshold = 0.5):\n",
    "  \n",
    "  training_set_gender = []\n",
    "\n",
    "# Recorrer todo el training_set para encontrar las muestras mÃ¡s cercanas\n",
    "\n",
    "  for sample in training_set: \n",
    "    gender_prob, factual_age = mivolo_inference(sample, True)\n",
    "    if gender_prob >= 0.5:\n",
    "      pred_gender = 1\n",
    "    else:\n",
    "      pred_gender = 0\n",
    "    training_set_gender.append(pred_gender)\n",
    "      \n",
    "  return training_set_gender\n",
    "\n",
    "def gender_test_set(training_images, training_labels, coverage = 0.30):\n",
    "  \n",
    "  # Crear las matrices de distancias\n",
    "  \n",
    "  factual_to_training_distances, factual_to_training_indices = distances_between_image_sets(factual_image, img_size = 128, training_set = training_images, distance_metric = 'pixel_wise')\n",
    "  \n",
    "  # Seleccionar k ejemplos de la clase positiva y de la negativa\n",
    "  \n",
    "  n = len(training_labels)\n",
    "  n_female = np.sum(training_labels)\n",
    "  n_male = n - n_female\n",
    "  \n",
    "  m_male = round(n_male * coverage)\n",
    "  m_female = round(n_female * coverage)\n",
    "  \n",
    "  female_indices = torch.tensor([i for i, g in zip(factual_to_training_indices, training_labels) if training_labels[i] == 1][0:m_female])\n",
    "  male_indices = torch.tensor([i for i, g in zip(factual_to_training_indices, training_labels) if training_labels[i] == 0][0:m_male])\n",
    "    \n",
    "  male_samples = torch.index_select(training_images, 0, torch.tensor(male_indices))\n",
    "  female_samples = torch.index_select(training_images, 0, torch.tensor(female_indices))\n",
    "  \n",
    "  test_set_samples = torch.cat((female_samples, male_samples), dim = 0)\n",
    "  test_set_gender = torch.cat((torch.ones(m_female, 1), torch.zeros(m_male, 1)))\n",
    "  \n",
    "  closest_data_idx_male = male_indices[0]\n",
    "  closest_data_idx_female = female_indices[0]\n",
    "  return test_set_samples, test_set_gender, closest_data_idx_male, closest_data_idx_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b48f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets\n",
    "\n",
    "training_set_gender = gender_training_set()\n",
    "test_set_samples, test_set_gender, closest_data_idx_male, closest_data_idx_female = gender_test_set(training_set_images, training_set_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set for 1-NN Classifier\n",
    "\n",
    "training_set_knn = torch.concat([valid_cf_images, factual_image])\n",
    "training_set_gender_knn = gender_training_set(valid_cf_images)\n",
    "training_set_gender_knn.append(factual_gender_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(image, training_set_knn):\n",
    "  distances, indices = distances_between_image_sets(image, img_size = 128, training_set = training_set_knn, distance_metric = 'pixel_wise')\n",
    "  nearest_gender = training_set_gender_knn[indices[0]]\n",
    "  if nearest_gender >= 0.5:\n",
    "    p = 1\n",
    "  else:\n",
    "    p = 0\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Gender on test set\n",
    "\n",
    "test_set_predictions = []\n",
    "\n",
    "for sample in test_set_samples:\n",
    "  p = predict_gender(sample, training_set_knn)\n",
    "  test_set_predictions.append(p)\n",
    "  \n",
    "test_set_predictions = torch.tensor(test_set_predictions).unsqueeze(1)\n",
    "\n",
    "accuracy = torch.sum(test_set_predictions == test_set_gender) / test_set_gender.shape[0]\n",
    "evaluation_metrics[\"Discriminative Power\"] = {\"1nn_accuracy\":{\"value\":round(accuracy.item(), 3), \"definition\":\"Accuracy of a 1-NN classifier trained on CFs + 1F, evaluated over k members of F and CF classes\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime\n",
    "\n",
    "evaluation_metrics[\"Runtime\"] = {\"execution_time\":{\"value\":round(runtime_in_seconds, 3), \"definition\":\"Algorithm's Runtime\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca12450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implausibility - Average distance of the CF from the closest instance in the known set X\n",
    "\n",
    "min_distances = torch.tensor(0)\n",
    "for cf in valid_cf_images:\n",
    "  distances, indices = distances_between_image_sets(cf, img_size = 128, training_set = training_set_images, distance_metric = 'lpips')\n",
    "  min_d = distances[0]\n",
    "  min_distances = min_distances + min_d\n",
    "average_min_distance = min_distances / valid_cf_images.shape[0]\n",
    "\n",
    "evaluation_metrics[\"Implausibility\"] = {\n",
    "  \"average_min_distance_training\":{\"value\":round(average_min_distance.item(), 3), \"definition\":\"Average distance of the CFs from their closest instance in the training set\"}\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image for the instability metric\n",
    "\n",
    "if factual_gender_binary == 1:\n",
    "  sample_idx = closest_data_idx_female\n",
    "else:\n",
    "  sample_idx = closest_data_idx_male\n",
    "\n",
    "instability_image = training_set_images[sample_idx].unsqueeze(0)\n",
    "instability_image_atts = training_set_attributes[sample_idx]\n",
    "instability_gender_prob, instability_age = mivolo_inference(instability_image, True)\n",
    "if instability_gender_prob > 0.5:\n",
    "  instability_gender_binary = 1\n",
    "else:\n",
    "  instability_gender_binary = 0\n",
    "  \n",
    "# Obtain the predicted attributes for the factual image\n",
    "\n",
    "with torch.no_grad():\n",
    "  instability_d_score, dc_instability = attgan.D(instability_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4734b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instability\n",
    "\n",
    "# Generate CFs for the closest sample with the same classification as the factual image\n",
    "\n",
    "desired_pred = 1 - instability_gender_binary\n",
    "max_evals = 1000\n",
    "pop_size = 100\n",
    "\n",
    "problem = AttGanPlausibleCounterfactualProblem(\n",
    "            image = instability_image, \n",
    "            code = dc_instability, # use the predicted scores for each attribute\n",
    "            decoder = attgan.G, \n",
    "            discriminator = attgan.D, \n",
    "            classifier = mivolo_inference, \n",
    "            original_pred = instability_gender_prob,\n",
    "            original_discriminator_score = instability_d_score,\n",
    "            desired_pred = desired_pred,\n",
    "            use_lpips = True\n",
    "          )\n",
    "\n",
    "algorithm = NSGAII(\n",
    "             problem=problem,\n",
    "             population_size=pop_size,\n",
    "             offspring_population_size=pop_size,\n",
    "             mutation=PolynomialMutation(\n",
    "                 probability=1/problem._number_of_variables,\n",
    "                 distribution_index=20),\n",
    "             crossover=SBXCrossover(probability=1.0, distribution_index=20),\n",
    "             termination_criterion=StoppingByEvaluations(max_evaluations=max_evals)\n",
    "         )\n",
    "        \n",
    "algorithm.run()\n",
    "pareto_front = algorithm.result()\n",
    "\n",
    "instability_attributes_set = []\n",
    "\n",
    "for sol in pareto_front:\n",
    "    x = float(sol.objectives[0])\n",
    "    y = float(sol.objectives[1])\n",
    "    z = float(sol.objectives[2])\n",
    "    new_code = torch.tensor(sol.variables)\n",
    "    new_pred = sol.prediction\n",
    "    if np.isfinite(x) and np.isfinite(y) and np.isfinite(z):\n",
    "      if y < 0.5: # CF validos\n",
    "        instability_attributes_set.append(new_code)\n",
    "      \n",
    "instability_attributes_set = torch.stack(instability_attributes_set)\n",
    "\n",
    "d, _ = distances_between_image_sets(instability_image, img_size = 128, training_set = factual_image, distance_metric = 'pixel_wise')\n",
    "instability_score = 0\n",
    "for cf in valid_cf_atts:\n",
    "  distance_to_cf = torch.sqrt(((cf - instability_attributes_set) ** 2).sum(-1))\n",
    "  instability_score += distance_to_cf\n",
    "instability_score = torch.sum(instability_score) / (instability_attributes_set.shape[0] * valid_cf_atts.shape[0]) * 1 / (1 + d)\n",
    "\n",
    "evaluation_metrics[\"Instability\"] = {\"instability\":{\"value\":round(instability_score.item(), 3), \"definition\":\"Obtain CFs for the closest sample with the same classification as X. Measure the distance between these sets\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FID score between CFs and training samples\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(normalize = True)\n",
    "fid.update(normalize_0_1(training_set_images), real=True)\n",
    "fid.update(normalize_0_1(valid_cf_images), real=False)\n",
    "fid_score = fid.compute()\n",
    "\n",
    "evaluation_metrics[\"FID_score\"] = {\"fid\":{\"value\":round(fid_score.item(), 3), \"definition\":\"FID score between valid CFs and training set\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd36874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypervolume\n",
    "\n",
    "hv = hypervolume_indicator(pareto_front)\n",
    "\n",
    "evaluation_metrics[\"Hypervolume\"] = {\n",
    "  \"hypervolume\":{\"value\":round(hv.item(), 3), \"definition\":\"Hypervolume indicator of the estimated Pareto front wrt the point (1, 1, 1)\"}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabar las metricas en excel \n",
    "\n",
    "metrics_df = pd.DataFrame([], columns=['Metric_Class', 'Metric_SubClass', 'Metric_Name', 'Value', 'Definition'])\n",
    "for key in evaluation_metrics.keys():\n",
    "    for sub_key in evaluation_metrics[key]:\n",
    "        if 'value' in evaluation_metrics[key][sub_key]:\n",
    "            value = evaluation_metrics[key][sub_key][\"value\"]\n",
    "            definition = evaluation_metrics[key][sub_key][\"definition\"]\n",
    "            name = sub_key\n",
    "            metric_class = key\n",
    "            metric_subclass = None\n",
    "            row = pd.DataFrame([[metric_class, metric_subclass, name, value, definition]], columns=['Metric_Class', 'Metric_SubClass', 'Metric_Name', 'Value', 'Definition'])\n",
    "            metrics_df = pd.concat((metrics_df, row))\n",
    "        else:\n",
    "            for subkey2 in evaluation_metrics[key][sub_key]:\n",
    "                if 'value' in evaluation_metrics[key][sub_key][subkey2]:\n",
    "                    value = evaluation_metrics[key][sub_key][subkey2][\"value\"]\n",
    "                    definition = evaluation_metrics[key][sub_key][subkey2][\"definition\"]\n",
    "                    name = subkey2\n",
    "                    metric_class = key\n",
    "                    metric_subclass = sub_key\n",
    "                    row = pd.DataFrame([[metric_class, metric_subclass, name, value, definition]], columns=['Metric_Class', 'Metric_SubClass', 'Metric_Name', 'Value', 'Definition'])\n",
    "                    metrics_df = pd.concat((metrics_df, row))\n",
    "\n",
    "for key in experiment_metadata:\n",
    "    metric_class = 'Metadata'\n",
    "    metric_subclass = 'Hyperparameter'\n",
    "    name = key\n",
    "    value = experiment_metadata[key]\n",
    "    definition = None\n",
    "    row = pd.DataFrame([[metric_class, metric_subclass, name, value, definition]], columns=['Metric_Class', 'Metric_SubClass', 'Metric_Name', 'Value', 'Definition'])\n",
    "    metrics_df = pd.concat((metrics_df, row))\n",
    "    \n",
    "metrics_df.to_excel('./Counterfactuals/' + path.split('/')[2].replace('.pkl', '.xlsx'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "count-gen-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

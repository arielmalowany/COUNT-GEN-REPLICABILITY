{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00569e0c",
   "metadata": {},
   "source": [
    "### Importar librerias, configuraciones y la ATT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc28f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 112 layers, 68,125,494 parameters, 0 gradients, 257.4 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nn import LinearBlock, Conv2dBlock, ConvTranspose2dBlock\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import torch.utils.data as data\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import torchvision.transforms as transforms\n",
    "from jmetal.algorithm.multiobjective import NSGAII\n",
    "from jmetal.operator import SBXCrossover, PolynomialMutation\n",
    "from jmetal.util.termination_criterion import StoppingByEvaluations\n",
    "from jmetal.util.termination_criterion import StoppingByQualityIndicator\n",
    "from jmetal.util.observer import ProgressBarObserver\n",
    "from jmetal.core.problem import FloatProblem\n",
    "from jmetal.core.solution import FloatSolution\n",
    "from jmetal.lab.experiment import Experiment, Job, generate_summary_from_experiment\n",
    "from jmetal.core.quality_indicator import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from jmetal.core.quality_indicator import HyperVolume\n",
    "import os\n",
    "import inspect\n",
    "from jmetal.util.ranking import FastNonDominatedRanking\n",
    "from jmetal.util.density_estimator import CrowdingDistance\n",
    "from jmetal.operator.selection import BinaryTournamentSelection\n",
    "from jmetal.util.comparator import MultiComparator\n",
    "from jmetal.util.comparator import DominanceWithConstraintsComparator\n",
    "\n",
    "# custom scripts\n",
    "from my_attgan import AttGAN\n",
    "from my_mivolo_inference import mivolo_inference\n",
    "from my_mivolo_inference import predictor\n",
    "from cf_utils import *\n",
    "from generate_gender_cfs import AttGanPlausibleCounterfactualProblem\n",
    "from data import Custom\n",
    "from data import CelebA_HQ_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f799a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings and base att names\n",
    "\n",
    "with open('./384_shortcut1_inject1_none_hq/setting.txt', 'r') as f:\n",
    "    gan_args = json.load(f)\n",
    "    \n",
    "base_attrs = gan_args.get('attrs')\n",
    "\n",
    "# Load AttGAN\n",
    "\n",
    "attgan = AttGAN(gan_args)\n",
    "attgan.load('./384_shortcut1_inject1_none_hq/weights.149.pth')\n",
    "attgan.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b4cdc",
   "metadata": {},
   "source": [
    "### Cargar los datos para generar los CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9aefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image = 'from_front'\n",
    "path = './Counterfactuals/Front_10374_0.pkl'\n",
    "\n",
    "if custom_image == 'custom':\n",
    "\n",
    "  # Load dataloader ATTGAN repo\n",
    "\n",
    "  test_dataset = Custom('./data/custom', './data/list_attr_custom.txt', gan_args.get('img_size'), gan_args.get('attrs'))\n",
    "  test_dataloader = data.DataLoader(\n",
    "      test_dataset, batch_size=1, num_workers=gan_args.get('num_workers'),\n",
    "      shuffle=False, drop_last=False\n",
    "  )\n",
    "  # Normalizes data using mean 0.5 and std 0.5 -> range [-1, 1]\n",
    "\n",
    "  # Test Data\n",
    "  \n",
    "  training_set_images = torch.tensor([])\n",
    "  training_set_attributes = torch.tensor([])\n",
    "  training_set_file_names = []\n",
    "  \n",
    "  for idx, (img_a, att_a, file_nm) in enumerate(test_dataloader):\n",
    "    training_set_images = torch.cat((training_set_images, img_a), dim = 0)\n",
    "    training_set_attributes = torch.cat((training_set_attributes, att_a), dim = 0)\n",
    "    training_set_file_names.append(file_nm)\n",
    "    \n",
    "  # Receives PIL Image (384, 384, 3) x (0, 255)\n",
    "  # To Tensor scales to (0, 1)\n",
    "  # Normalize scales to (-1, 1) [(0 - 0.5)/0.5, (1 - 0.5)/0.5]\n",
    "  # Outputs tensor (1, 3, 384, 384) x (-1, 1)\n",
    "  \n",
    "elif custom_image == 'random':\n",
    "  \n",
    "  # CelebaHQ N samples\n",
    "\n",
    "  celeba_path = './celeba_hq_dataset/CelebA-HQ-img'\n",
    "  atts_path = './celeba_hq_dataset/CelebAMask-HQ-attribute-anno.txt'\n",
    "  base_attrs = gan_args.get('attrs')\n",
    "\n",
    "  sample_celeba_data = CelebA_HQ_custom(\n",
    "                        data_path = celeba_path,\n",
    "                        attr_path = atts_path,\n",
    "                        selected_attrs = base_attrs,\n",
    "                        image_size = gan_args.get('img_size'),\n",
    "                        mode = 'train'\n",
    "                      )\n",
    "\n",
    "  sample_celeba_dataloader = data.DataLoader(\n",
    "                              sample_celeba_data, batch_size= 1, num_workers=gan_args.get('num_workers'),\n",
    "                              shuffle=True, drop_last=False\n",
    "                            )\n",
    "  \n",
    "  data_iterator = iter(sample_celeba_dataloader)\n",
    "  training_set_images, training_set_attributes, training_set_file_names = next(data_iterator)\n",
    "  \n",
    "elif custom_image == 'from_front':\n",
    "  with open(path, 'rb') as f:\n",
    "    pareto_front = pkl.load(f)\n",
    "    training_set_images = pkl.load(f)\n",
    "    training_set_attributes = pkl.load(f)\n",
    "    training_set_file_names = [(str.split(path, '_')[1] + '.jpg', )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8e476",
   "metadata": {},
   "source": [
    "### Generar los CFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008320ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9972637891769409 0\n"
     ]
    }
   ],
   "source": [
    "# Define factual image\n",
    "\n",
    "null_intervention = False\n",
    "\n",
    "sample_idx = 0\n",
    "factual_img = torch.index_select(training_set_images, 0, torch.tensor(sample_idx))\n",
    "factual_atts = torch.index_select(training_set_attributes, 0, torch.tensor(sample_idx))\n",
    "img_file_name = training_set_file_names[sample_idx][0]\n",
    "d_real, dc_real = attgan.D(factual_img)\n",
    "factual_img_recons = attgan.G(factual_img, dc_real)\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', reduction='none')\n",
    "lpips_recons = lpips(factual_img, factual_img_recons).item()\n",
    "\n",
    "prediction_orig, _ = mivolo_inference(factual_img, True)\n",
    "\n",
    "if prediction_orig >= 0.5:\n",
    "  desired_pred = 0\n",
    "else:\n",
    "  desired_pred = 1\n",
    "  \n",
    "if null_intervention:\n",
    "  desired_pred = 1 - desired_pred\n",
    "  \n",
    "print(prediction_orig, desired_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7258997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-11 15:52:16,211] [jmetal.core.algorithm] [DEBUG] Creating initial set of solutions...\n",
      "[2025-12-11 15:52:16,212] [jmetal.core.algorithm] [DEBUG] Evaluating solutions...\n",
      "[2025-12-11 15:53:24,968] [jmetal.core.algorithm] [DEBUG] Initializing progress...\n",
      "Progress:   0%|          | 0/5000 [00:00<?, ?it/s][2025-12-11 15:53:24,975] [jmetal] [INFO] Evaluations: 100 \n",
      " HyperVolume: 0.8272536355201469 \n",
      " Computing time: 68.75938892364502\n",
      "[2025-12-11 15:53:25,364] [jmetal.core.algorithm] [DEBUG] Running main loop until termination criteria is met\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m hipervolume_indicator \u001b[38;5;241m=\u001b[39m HyperVolumeObserver(frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, img_file_name \u001b[38;5;241m=\u001b[39m img_file_name)\n\u001b[1;32m     53\u001b[0m algorithm\u001b[38;5;241m.\u001b[39mobservable\u001b[38;5;241m.\u001b[39mregister(hipervolume_indicator)\n\u001b[0;32m---> 55\u001b[0m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m pareto_front \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     57\u001b[0m runtime_in_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(algorithm\u001b[38;5;241m.\u001b[39mtotal_computing_time, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/core/algorithm.py:86\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning main loop until termination criteria is met\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopping_condition_is_met():\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_progress()\n\u001b[1;32m     89\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/core/algorithm.py:145\u001b[0m, in \u001b[0;36mEvolutionaryAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 145\u001b[0m     mating_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     offspring_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreproduction(mating_population)\n\u001b[1;32m    147\u001b[0m     offspring_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(offspring_population)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/algorithm/singleobjective/genetic_algorithm.py:76\u001b[0m, in \u001b[0;36mGeneticAlgorithm.selection\u001b[0;34m(self, population)\u001b[0m\n\u001b[1;32m     73\u001b[0m mating_population \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmating_pool_size):\n\u001b[0;32m---> 76\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselection_operator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     mating_population\u001b[38;5;241m.\u001b[39mappend(solution)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mating_population\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/operator/selection.py:70\u001b[0m, in \u001b[0;36mBinaryTournamentSelection.execute\u001b[0;34m(self, front)\u001b[0m\n\u001b[1;32m     67\u001b[0m solution1 \u001b[38;5;241m=\u001b[39m front[i]\n\u001b[1;32m     68\u001b[0m solution2 \u001b[38;5;241m=\u001b[39m front[j]\n\u001b[0;32m---> 70\u001b[0m flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m solution1\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/util/comparator.py:91\u001b[0m, in \u001b[0;36mMultiComparator.compare\u001b[0;34m(self, solution1, solution2)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompare\u001b[39m(\u001b[38;5;28mself\u001b[39m, solution1: Solution, solution2: Solution) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comparator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomparator_list:\n\u001b[0;32m---> 91\u001b[0m         flag \u001b[38;5;241m=\u001b[39m \u001b[43mcomparator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m flag\n",
      "File \u001b[0;32m/opt/anaconda3/envs/count-gen-demo/lib/python3.10/site-packages/jmetal/util/comparator.py:57\u001b[0m, in \u001b[0;36mSolutionAttributeComparator.compare\u001b[0;34m(self, solution1, solution2)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompare\u001b[39m(\u001b[38;5;28mself\u001b[39m, solution1: Solution, solution2: Solution) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     value1 \u001b[38;5;241m=\u001b[39m \u001b[43msolution1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m     58\u001b[0m     value2 \u001b[38;5;241m=\u001b[39m solution2\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m     60\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Generate CFs\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "pop_size = 100\n",
    "max_evals = 5000\n",
    "use_lpips = False\n",
    "\n",
    "if use_lpips:\n",
    "  original_discriminator_score = lpips_recons\n",
    "else:\n",
    "  original_discriminator_score = d_real.detach()\n",
    "\n",
    "# Solver \n",
    "\n",
    "problem = AttGanPlausibleCounterfactualProblem(\n",
    "            image = factual_img, \n",
    "            code = dc_real, # use the predicted scores for each attribute\n",
    "            decoder = attgan.G, \n",
    "            discriminator = attgan.D, \n",
    "            classifier = mivolo_inference, \n",
    "            original_pred = prediction_orig,\n",
    "            original_discriminator_score = original_discriminator_score,\n",
    "            desired_pred = desired_pred,\n",
    "            use_lpips = use_lpips,\n",
    "            non_actionable_features = None\n",
    "          )\n",
    "\n",
    "algorithm = NSGAII(\n",
    "             problem=problem,\n",
    "             population_size=pop_size,\n",
    "             offspring_population_size=pop_size,\n",
    "             selection = BinaryTournamentSelection(\n",
    "                           MultiComparator([DominanceWithConstraintsComparator(), CrowdingDistance.get_comparator()])\n",
    "                         ),\n",
    "             mutation=PolynomialMutation(\n",
    "                 probability=1/problem._number_of_variables,\n",
    "                 distribution_index=20),\n",
    "             crossover=SBXCrossover(probability=0.9, distribution_index=20),\n",
    "             termination_criterion=StoppingByEvaluations(max_evaluations=max_evals),\n",
    "             dominance_comparator = DominanceWithConstraintsComparator()\n",
    "         )\n",
    "\n",
    "#jobs = [Job(algorithm, algorithm_tag = 'NSGAII', problem_tag = 'Gender_CF', run = max_evals)]\n",
    "#experiment = Experiment(output_dir='./Counterfactuals', jobs=jobs)\n",
    "#pareto_front = experiment.jobs[0].algorithm.result()\n",
    "\n",
    "progress_bar = ProgressBarObserver(max = max_evals)\n",
    "algorithm.observable.register(progress_bar)\n",
    "hipervolume_indicator = HyperVolumeObserver(frequency=100, img_file_name = img_file_name)\n",
    "algorithm.observable.register(hipervolume_indicator)\n",
    "        \n",
    "algorithm.run()\n",
    "pareto_front = algorithm.result()\n",
    "runtime_in_seconds = round(algorithm.total_computing_time, 3)\n",
    "\n",
    "write_pkl_file(img_file_name, algorithm, problem, factual_img, factual_atts, pareto_front, runtime_in_seconds, prediction_orig, desired_pred, pop_size, max_evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "count-gen-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

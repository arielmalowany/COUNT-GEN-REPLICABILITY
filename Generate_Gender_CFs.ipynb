{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00569e0c",
   "metadata": {},
   "source": [
    "### Importar librerias, configuraciones y la ATT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc28f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 112 layers, 68,125,494 parameters, 0 gradients, 257.4 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nn import LinearBlock, Conv2dBlock, ConvTranspose2dBlock\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import torch.utils.data as data\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import torchvision.transforms as transforms\n",
    "from jmetal.algorithm.multiobjective import NSGAII\n",
    "from jmetal.operator import SBXCrossover, PolynomialMutation\n",
    "from jmetal.util.termination_criterion import StoppingByEvaluations\n",
    "from jmetal.util.termination_criterion import StoppingByQualityIndicator\n",
    "from jmetal.util.observer import ProgressBarObserver\n",
    "from jmetal.core.problem import FloatProblem\n",
    "from jmetal.core.solution import FloatSolution\n",
    "from jmetal.lab.experiment import Experiment, Job, generate_summary_from_experiment\n",
    "from jmetal.core.quality_indicator import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from jmetal.core.quality_indicator import HyperVolume\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "# custom scripts\n",
    "from my_attgan import AttGAN\n",
    "from my_mivolo_inference import mivolo_inference\n",
    "from my_mivolo_inference import predictor\n",
    "from cf_utils import *\n",
    "from generate_gender_cfs import AttGanPlausibleCounterfactualProblem\n",
    "from data import Custom\n",
    "from data import CelebA_HQ_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f799a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings and base att names\n",
    "\n",
    "with open('./384_shortcut1_inject1_none_hq/setting.txt', 'r') as f:\n",
    "    gan_args = json.load(f)\n",
    "    \n",
    "base_attrs = gan_args.get('attrs')\n",
    "\n",
    "# Load AttGAN\n",
    "\n",
    "attgan = AttGAN(gan_args)\n",
    "attgan.load('./384_shortcut1_inject1_none_hq/weights.149.pth')\n",
    "attgan.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b4cdc",
   "metadata": {},
   "source": [
    "### Cargar los datos para generar los CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b9aefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image = 'from_front'\n",
    "path = './Counterfactuals/Front_13053_0.pkl'\n",
    "\n",
    "if custom_image == 'custom':\n",
    "\n",
    "  # Load dataloader ATTGAN repo\n",
    "\n",
    "  test_dataset = Custom('./data/custom', './data/list_attr_custom.txt', gan_args.get('img_size'), gan_args.get('attrs'))\n",
    "  test_dataloader = data.DataLoader(\n",
    "      test_dataset, batch_size=1, num_workers=gan_args.get('num_workers'),\n",
    "      shuffle=False, drop_last=False\n",
    "  )\n",
    "  # Normalizes data using mean 0.5 and std 0.5 -> range [-1, 1]\n",
    "\n",
    "  # Test Data\n",
    "  \n",
    "  training_set_images = torch.tensor([])\n",
    "  training_set_attributes = torch.tensor([])\n",
    "  training_set_file_names = []\n",
    "  \n",
    "  for idx, (img_a, att_a, file_nm) in enumerate(test_dataloader):\n",
    "    training_set_images = torch.cat((training_set_images, img_a), dim = 0)\n",
    "    training_set_attributes = torch.cat((training_set_attributes, att_a), dim = 0)\n",
    "    training_set_file_names.append(file_nm)\n",
    "    \n",
    "  # Receives PIL Image (384, 384, 3) x (0, 255)\n",
    "  # To Tensor scales to (0, 1)\n",
    "  # Normalize scales to (-1, 1) [(0 - 0.5)/0.5, (1 - 0.5)/0.5]\n",
    "  # Outputs tensor (1, 3, 384, 384) x (-1, 1)\n",
    "  \n",
    "elif custom_image == 'random':\n",
    "  \n",
    "  # CelebaHQ N samples\n",
    "\n",
    "  celeba_path = './celeba_hq_dataset/CelebA-HQ-img'\n",
    "  atts_path = './celeba_hq_dataset/CelebAMask-HQ-attribute-anno.txt'\n",
    "  base_attrs = gan_args.get('attrs')\n",
    "\n",
    "  sample_celeba_data = CelebA_HQ_custom(\n",
    "                        data_path = celeba_path,\n",
    "                        attr_path = atts_path,\n",
    "                        selected_attrs = base_attrs,\n",
    "                        image_size = gan_args.get('img_size'),\n",
    "                        mode = 'train'\n",
    "                      )\n",
    "\n",
    "  sample_celeba_dataloader = data.DataLoader(\n",
    "                              sample_celeba_data, batch_size= 1, num_workers=gan_args.get('num_workers'),\n",
    "                              shuffle=True, drop_last=False\n",
    "                            )\n",
    "  \n",
    "  data_iterator = iter(sample_celeba_dataloader)\n",
    "  training_set_images, training_set_attributes, training_set_file_names = next(data_iterator)\n",
    "  \n",
    "elif custom_image == 'from_front':\n",
    "  with open(path, 'rb') as f:\n",
    "    pareto_front = pkl.load(f)\n",
    "    training_set_images = pkl.load(f)\n",
    "    training_set_attributes = pkl.load(f)\n",
    "    training_set_file_names = [(str.split(path, '_')[1] + '.jpg', )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8e476",
   "metadata": {},
   "source": [
    "### Generar los CFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "008320ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define factual image\n",
    "\n",
    "null_intervention = True\n",
    "\n",
    "sample_idx = 0\n",
    "factual_img = torch.index_select(training_set_images, 0, torch.tensor(sample_idx))\n",
    "factual_atts = torch.index_select(training_set_attributes, 0, torch.tensor(sample_idx))\n",
    "img_file_name = training_set_file_names[sample_idx][0]\n",
    "d_real, dc_real = attgan.D(factual_img)\n",
    "prediction_orig, _ = mivolo_inference(factual_img, True)\n",
    "\n",
    "if prediction_orig >= 0.5:\n",
    "  desired_pred = 0\n",
    "else:\n",
    "  desired_pred = 1\n",
    "  \n",
    "if null_intervention:\n",
    "  desired_pred = 1 - desired_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7258997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 08:59:57,102] [jmetal.core.algorithm] [DEBUG] Creating initial set of solutions...\n",
      "[2025-11-26 08:59:57,103] [jmetal.core.algorithm] [DEBUG] Evaluating solutions...\n",
      "[2025-11-26 09:01:13,258] [jmetal.core.algorithm] [DEBUG] Initializing progress...\n",
      "Progress:   0%|          | 0/25000 [00:00<?, ?it/s][2025-11-26 09:01:13,292] [jmetal.core.algorithm] [DEBUG] Running main loop until termination criteria is met\n",
      "Progress: 100%|##########| 25000/25000 [5:57:19<00:00,  1.17it/s]  \n",
      "[2025-11-26 14:58:32,662] [jmetal.core.algorithm] [DEBUG] Finished!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Generate CFs\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "pop_size = 100\n",
    "max_evals = 25000\n",
    "\n",
    "# Solver \n",
    "\n",
    "problem = AttGanPlausibleCounterfactualProblem(\n",
    "            image = factual_img, \n",
    "            code = dc_real, # use the predicted scores for each attribute\n",
    "            decoder = attgan.G, \n",
    "            discriminator = attgan.D, \n",
    "            classifier = mivolo_inference, \n",
    "            original_pred = prediction_orig,\n",
    "            original_discriminator_score = d_real,\n",
    "            desired_pred = desired_pred,\n",
    "            use_lpips = True,\n",
    "            non_actionable_features = None\n",
    "          )\n",
    "\n",
    "algorithm = NSGAII(\n",
    "             problem=problem,\n",
    "             population_size=pop_size,\n",
    "             offspring_population_size=pop_size,\n",
    "             mutation=PolynomialMutation(\n",
    "                 probability=1/problem._number_of_variables,\n",
    "                 distribution_index=20),\n",
    "             crossover=SBXCrossover(probability=1.0, distribution_index=20),\n",
    "             termination_criterion=StoppingByEvaluations(max_evaluations=max_evals)\n",
    "         )\n",
    "\n",
    "#jobs = [Job(algorithm, algorithm_tag = 'NSGAII', problem_tag = 'Gender_CF', run = max_evals)]\n",
    "#experiment = Experiment(output_dir='./Counterfactuals', jobs=jobs)\n",
    "#pareto_front = experiment.jobs[0].algorithm.result()\n",
    "\n",
    "progress_bar = ProgressBarObserver(max=max_evals)\n",
    "algorithm.observable.register(progress_bar)\n",
    "        \n",
    "algorithm.run()\n",
    "pareto_front = algorithm.result()\n",
    "runtime_in_seconds = round(algorithm.total_computing_time, 3)\n",
    "\n",
    "write_pkl_file(img_file_name, algorithm, problem, factual_img, factual_atts, pareto_front, runtime_in_seconds, desired_pred, pop_size, max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba1257b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pkl_file(img_file_name, algorithm, problem, factual_img, factual_atts, pareto_front, runtime_in_seconds, desired_pred, pop_size, max_evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "count-gen-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
